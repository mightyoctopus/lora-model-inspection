{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOzAh+yj+55LqRlXrKYZJga",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mightyoctopus/lora-model-inspection/blob/main/w7_d1_LoRA_Intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict Product Prices\n",
        "\n",
        "An introduction to LoRA and QLoRA.\n",
        "\n",
        "Take a close look at the footprint memory for each model and model architecture."
      ],
      "metadata": {
        "id": "HRJU5Yacs_28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install -q requests bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0\n",
        "!pip install -q datasets requests peft"
      ],
      "metadata": {
        "id": "MiPYIJLKrLxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, set_seed\n",
        "from peft import LoraConfig, PeftModel\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "9OtOcMT8FeQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Constants\n",
        "\n",
        "BASE_MODEL = \"meta-llama/Meta-Llama-3.1-8B\"\n",
        "FINETUNED_MODEL = f\"ed-donner/pricer-2024-09-13_13.04.39\"\n",
        "\n",
        "\n",
        "### HyperParameters for QLoRA\n",
        "\n",
        "LORA_R = 32\n",
        "LORA_ALPHA = 64\n",
        "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]"
      ],
      "metadata": {
        "id": "kx1JiBMvGA1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Log in to Hugging Face\n",
        "\n",
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "Y61qo8BrIX6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Load the base model (without quantization yet)\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map=\"auto\")"
      ],
      "metadata": {
        "id": "WRHFvLOTJGb1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Memory Footprint: {base_model.get_memory_footprint() / 1e9:,.1f} GB\")"
      ],
      "metadata": {
        "id": "PN-kZhsVRcQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model"
      ],
      "metadata": {
        "id": "b-Wac2v1Srtz",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Restart the session\n",
        "\n",
        "The model will be loaded with quantization 8 bit (Simplified quantization, not the full configuration)"
      ],
      "metadata": {
        "id": "J7kTp7JwVS4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Load the base model using 8 bit\n",
        "\n",
        "quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "fd-5yA9_VdeY",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Footprint Memory: {base_model.get_memory_footprint() / 1e9:,.1f}GB\")"
      ],
      "metadata": {
        "id": "GMuciYsv0oTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model"
      ],
      "metadata": {
        "id": "OcB4QhJx225g",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Restart the session\n",
        "\n",
        "The model will be loaded with quantization 4 bit (with full configuration)"
      ],
      "metadata": {
        "id": "C6t_hAcs4-uh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Load the base model with 4 bit quantization\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "sRAoszSU5Iq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Check Memory Footprint of the model (with 4 bit)\n",
        "\n",
        "print(f\"Memory Footprint: {base_model.get_memory_footprint() / 1e9:,.2f}GB\")"
      ],
      "metadata": {
        "id": "iZA9wqFf6h7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model"
      ],
      "metadata": {
        "id": "XfPTXCRN654p",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Restart the session\n",
        "\n",
        "A fine tuned model with 4 bit quantization and will be checked the model architecture"
      ],
      "metadata": {
        "id": "OT53ST-ByhTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_oVRWbpVy28b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Memory Footprint: {base_model.get_memory_footprint() / 1e9:,.2f} GB\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "j-7Pa4bI0Op8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model"
      ],
      "metadata": {
        "collapsed": true,
        "id": "M40QJ-r804C9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Loading a fine tuned model\n",
        "\n",
        "fine_tuned_model = PeftModel.from_pretrained(base_model,FINETUNED_MODEL)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "QksKBFg205gS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Memory Footprint: {fine_tuned_model.get_memory_footprint() / 1e9:,.2f} GB\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "yO8Oaw7I1GKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_model"
      ],
      "metadata": {
        "collapsed": true,
        "id": "0Q4Cq9M71b8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Each of the Target Modules has 2 LoRA Adaptor matrices, called lora_A and lora_B\n",
        "### These are designed so that weights can be adapted by adding alpha * lora_A * lora_B\n",
        "### Let's count the number of weights using their dimensions:\n",
        "\n",
        "in_f = 4096\n",
        "\n",
        "lora_q_proj = LORA_R * in_f + LORA_R * 4096\n",
        "lora_k_proj = LORA_R * in_f + LORA_R * 1024\n",
        "lora_v_proj = LORA_R * in_f + LORA_R * 1024\n",
        "lora_o_proj = LORA_R * in_f + LORA_R * 4096\n",
        "\n",
        "lora_layer = lora_q_proj + lora_k_proj + lora_v_proj + lora_o_proj\n",
        "\n",
        "print(f\"{lora_layer:,} parameters per layer\")\n",
        "\n",
        "\n",
        "# There are 32 layers\n",
        "params = lora_layer * 32\n",
        "\n",
        "print(f\"{params:,} parameters\")\n",
        "\n",
        "\n",
        "### Total size in MB:\n",
        "size = (params * 4) / 1_000_000\n",
        "print(f\"{size:.2f} MB\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5dYPpjVsgF-n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}